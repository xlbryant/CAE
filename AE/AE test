#model AE / DAE 都是全连接
#noise_rate  噪声大小
#structure   输入  隐层单元数量... 输出 
#learning_rate 学习率
#val_loss 验证集loss
#optm = AdamOptimizer
model   noise_rate  structure        learning_rate   val_loss
DAE     0.3         8 4 4 8          0.001           0.06141
DAE     0.2         8 4 4 8          0.001           0.05776
DAE     0.1         8 4 4 8          0.001           0.05556
DAE     0.05        8 4 4 8          0.001           0.05484   
DAE     0.05        8 4 4 8          0.0001          0.05499
DAE     0.01        8 4 4 8          0.001           0.054358
DAE     0.005       8 4 4 8          0.001           0.054325
DAE     0.001       8 4 4 8          0.001           0.054348
DAE     0.1         8 4 4 8          0.0001          0.05570
DAE     0.01        8 4 2 2 4 8      0.001           0.05728
DAE     0.01        8 6 4 4 6 8      0.001           0.05431

AE                  8 4 4 8          0.001           0.054291
AE                  8 4 4 8          0.0001          0.054731

AE                  8 4 2 2 4 8      0.001           0.094515

AE                  8 7 6 6 7 8      0.001           0.054084  
AE                  8 16 12 12 16 8  0.001           0.054005
AE                  8 6 4 4 6 8      0.001           0.054240   


#structure   输入  隐层单元数量... 输出 
#noise_rate 噪声大小，如果是0 可当做AE
#learning_rate = 0.001
#optm = AdamOptimizer
structure           noise_rate      val_loss1         val_loss2
8, 4, 4, 1, 1, 8,   0               0.0945251263678   0.057366
8, 4, 4, 1, 1, 8,   0.1             0.0565876975656   0.058347
8, 4, 4, 1, 1, 8,   0.01            0.0945223107934   0.094540 
8, 4, 4, 1, 1, 8,   0.001           0.0598610691726   0.094520
8, 4, 4, 1, 1, 8,   0.0001          0.0573072750121   0.057325
8, 4, 4, 1, 1, 8,   1e-05           0.094535600394    0.094515
8, 4, 4, 2, 2, 8,   0               0.0945071935654   0.094166
8, 4, 4, 2, 2, 8,   0.1             0.0579715162516   0.095270
8, 4, 4, 2, 2, 8,   0.01            0.0945150204003   0.057159
8, 4, 4, 2, 2, 8,   0.001           0.0945022836328   0.054714
8, 4, 4, 2, 2, 8,   0.0001          0.0945155777037   0.054565
8, 4, 4, 2, 2, 8,   1e-05           0.0572726998478   0.054657
8, 6, 6, 4, 4, 8,   0               0.0543882489204   0.054286
8, 6, 6, 4, 4, 8,   0.1             0.0552973598242   0.055294
8, 6, 6, 4, 4, 8,   0.01            0.0543045692146   0.054267
8, 6, 6, 4, 4, 8,   0.001           0.0542714443058   0.054293
8, 6, 6, 4, 4, 8,   0.0001          0.0543003793806   0.094033
8, 6, 6, 4, 4, 8,   1e-05           0.0542082361877   0.056779
8, 7, 7, 6, 6, 8,   0               0.0541246749461   0.054158
8, 7, 7, 6, 6, 8,   0.1             0.055286206305    0.055292
8, 7, 7, 6, 6, 8,   0.01            0.0541821155697   0.054169
8, 7, 7, 6, 6, 8,   0.001           0.0541595865041   0.054119
8, 7, 7, 6, 6, 8,   0.0001          0.0541415710002   0.054098
8, 7, 7, 6, 6, 8,   1e-05           0.0542632445693   0.054176

#learning_rate = 0.0001
#optm = AdamOptimizer
                    noise_rate      val_loss
8, 4, 4, 1, 1, 8,   0               0.0574211876839
8, 4, 4, 1, 1, 8,   0.1             0.0953921690583
8, 4, 4, 1, 1, 8,   0.01            0.0577072586864
8, 4, 4, 1, 1, 8,   0.001           0.0946083441377
8, 4, 4, 1, 1, 8,   0.0001          0.0945771917701
8, 4, 4, 1, 1, 8,   1e-05           0.0573889233172
8, 4, 4, 2, 2, 8,   0               0.0575205322355
8, 4, 4, 2, 2, 8,   0.1             0.0587073091418
8, 4, 4, 2, 2, 8,   0.01            0.055165264383
8, 4, 4, 2, 2, 8,   0.001           0.0945723451674
8, 4, 4, 2, 2, 8,   0.0001          0.0573483549058
8, 4, 4, 2, 2, 8,   1e-05           0.0545961063355
8, 6, 6, 4, 4, 8,   0               0.0543687846512
8, 6, 6, 4, 4, 8,   0.1             0.0952944263816
8, 6, 6, 4, 4, 8,   0.01            0.0546458799392
8, 6, 6, 4, 4, 8,   0.001           0.0545009486377
8, 6, 6, 4, 4, 8,   0.0001          0.0570858746767
8, 6, 6, 4, 4, 8,   1e-05           0.0543043103069
8, 7, 7, 6, 6, 8,   0               0.0541795950383
8, 7, 7, 6, 6, 8,   0.1             0.0553321670741
8, 7, 7, 6, 6, 8,   0.01            0.0543302871287
8, 7, 7, 6, 6, 8,   0.001           0.0542913727462
8, 7, 7, 6, 6, 8,   0.0001          0.0542079459876
8, 7, 7, 6, 6, 8,   1e-05           0.0542146973312

#learning_rate = 0.0001  
#optm = AdagraOptimizer
8, 4, 4, 1, 1, 8,   0               0.125384870172
8, 4, 4, 1, 1, 8,   0.1             0.127878686786
8, 4, 4, 1, 1, 8,   0.01            0.131008329242
8, 4, 4, 1, 1, 8,   0.001           0.127786956728
8, 4, 4, 1, 1, 8,   0.0001          0.126163616776
8, 4, 4, 1, 1, 8,   1e-05           0.125760278106
8, 4, 4, 2, 2, 8,   0               0.12598875314
8, 4, 4, 2, 2, 8,   0.1             0.125370599329
8, 4, 4, 2, 2, 8,   0.01            0.1273533158
8, 4, 4, 2, 2, 8,   0.001           0.128559305519
8, 4, 4, 2, 2, 8,   0.0001          0.126109799743
8, 4, 4, 2, 2, 8,   1e-05           0.125880809128
8, 6, 6, 4, 4, 8,   0               0.126105940342
8, 6, 6, 4, 4, 8,   0.1             0.125793530047
8, 6, 6, 4, 4, 8,   0.01            0.126005522162
8, 6, 6, 4, 4, 8,   0.001           0.126148467511
8, 6, 6, 4, 4, 8,   0.0001          0.126168999076
8, 6, 6, 4, 4, 8,   1e-05           0.125506487489
8, 7, 7, 6, 6, 8,   0               0.125821861625
8, 7, 7, 6, 6, 8,   0.1             0.124178313464
8, 7, 7, 6, 6, 8,   0.01            0.0974995389581
8, 7, 7, 6, 6, 8,   0.001           0.123851076514
8, 7, 7, 6, 6, 8,   0.0001          0.123678644001
8, 7, 7, 6, 6, 8,   1e-05           0.12610931024